{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "- http://scikit-learn.org/stable/\n",
    "- http://sklearn.apachecn.org/cn/0.19.0/\n",
    "- https://jakevdp.github.io/PythonDataScienceHandbook/05.00-machine-learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means\n",
    "\n",
    "**reference**: \n",
    "- explaining by Andrew Ng: https://www.youtube.com/watch?v=hDmNF9JG3lo\n",
    "- sklearn API: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans\n",
    "- k-means on digits： https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html#Example-1:-k-means-on-digits\n",
    "- K-means: http://sklearn.apachecn.org/cn/0.19.0/modules/clustering.html#k-means\n",
    "\n",
    "**optimization target**: minimize the SSE (sum of squared errors)\n",
    "\n",
    "![kmeans-optimization-target](assets/kmeans-optimization-target.png)\n",
    "\n",
    "**pseudo code**:\n",
    "```\n",
    "randomly initialize cluster centroids \n",
    "while any cluster centroids moved:\n",
    "    1. cluster assignment: clustering all data points to centroids based on the L2(convensionly) distance\n",
    "    2. centroids update: move centroids to the cluster's center (average of the class group data)\n",
    "```\n",
    "**code**:\n",
    "```\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "def find_clusters(X, n_clusters, rseed=2):\n",
    "    # 1. Randomly choose clusters\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    i = rng.permutation(X.shape[0])[:n_clusters]\n",
    "    centers = X[i]\n",
    "    \n",
    "    while True:\n",
    "        # 2a. Assign labels based on closest center\n",
    "        labels = pairwise_distances_argmin(X, centers)\n",
    "        \n",
    "        # 2b. Find new centers from means of points\n",
    "        new_centers = np.array([X[labels == i].mean(0)\n",
    "                                for i in range(n_clusters)])\n",
    "        \n",
    "        # 2c. Check for convergence\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "        centers = new_centers\n",
    "    \n",
    "    return centers, labels\n",
    "\n",
    "centers, labels = find_clusters(X, 4)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');\n",
    "```\n",
    "\n",
    "\n",
    "**key points**\n",
    "- solved by Lloyd’s algorithm, average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration.\n",
    "- In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available)\n",
    "- k-means possibly falls in local minima. That’s why it can be useful to restart it several times.\n",
    "- It responds poorly to elongated clusters, or manifolds with irregular shapes\n",
    "- Running a dimensionality reduction algorithm such as PCA prior to k-means clustering can alleviate this problem and speed up the computations\n",
    "- there are several choice to initialize centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classification\n",
    "reference:\n",
    "- https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8\n",
    ">举个例子，如果一种水果其具有红，圆，直径大概3英寸等特征，该水果可以被判定为是苹果。尽管这些特征相互依赖或者有些特征由其他特征决定，然而朴素贝叶斯分类器认为这些属性在判定该水果是否为苹果的概率分布上独立的\n",
    "- https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/\n",
    "\n",
    "![NB_sample](assets/NB_example.png)  \n",
    "![NB_pros_cons](assets/NB_pros_cons.png)\n",
    "- http://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of clustering\n",
    "- https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html\n",
    "> Purity is a simple and transparent evaluation measure. Normalized mutual information can be information-theoretically interpreted. The Rand index penalizes both false positive and false negative decisions during clustering. The F measure in addition supports differential weighting of these two types of errors.\n",
    "\n",
    "    - Purity:  \n",
    "    >![clustering_purity](assets/clustering_purity.png)  \n",
    "High purity is easy to achieve when the number of clusters is large - in particular, purity is 1 if each document gets its own cluster. Thus, we cannot use purity to trade off the quality of the clustering against the number of clusters.\n",
    "    - NMI:\n",
    "    >A measure that allows us to make this tradeoff is normalized mutual information or NMI :  \n",
    "    ![clustering_nmi1](assets/clustering_nmi1.png)  \n",
    "    ![clustering_nmi2](assets/clustering_nmi2.png)  \n",
    "    ![clustering_nmi3](assets/clustering_nmi3.png)  \n",
    "\n",
    "    - Rand index\n",
    "    >![clustering_rand_index](assets/clustering_rand_index.png)  \n",
    "A true positive (TP) decision assigns two similar documents to the same cluster, a true negative (TN) decision assigns two dissimilar documents to different clusters. There are two types of errors we can commit. A (FP) decision assigns two dissimilar documents to the same cluster. A (FN) decision assigns two similar documents to different clusters. The Rand index ( ) measures the percentage of decisions that are correct.\n",
    "    \n",
    "    - F Measure\n",
    "    >![clustering_f_measure](assets/clustering_f_measure.png)  \n",
    "The Rand index gives equal weight to false positives and false negatives. Separating similar documents is sometimes worse than putting pairs of dissimilar documents in the same cluster. We can use the F measure measuresperf to penalize false negatives more strongly than false positives by selecting a value $\\beta > 1$, thus giving more weight to recall.\n",
    "\n",
    "- http://scikit-learn.org/stable/modules/classes.html#clustering-metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees and Random Forests\n",
    "- Decision Tree Samples: http://sklearn.apachecn.org/cn/0.19.0/auto_examples/index.html#decision-trees\n",
    "- Random Forest API: http://sklearn.apachecn.org/cn/0.19.0/modules/ensemble.html#id8\n",
    "- Decision Tree explanation: http://sklearn.apachecn.org/cn/0.19.0/modules/tree.html#tree\n",
    "- Notes: https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html\n",
    ">**Decision Trees:**  \n",
    "![decison_tree_sample](assets/decision_tree_sample.png)  \n",
    "![decison_tree_steps](assets/decision_tree_steps.png)  \n",
    ">  \n",
    ">**Random Forest:**  \n",
    "Multiple overfitting estimators can be combined to reduce the effect of this overfitting—is what underlies an ensemble method called bagging. Bagging makes use of an ensemble (a grab bag, perhaps) of parallel estimators, each of which over-fits the data, and averages the results to find a better classification. An ensemble of randomized decision trees is known as a random forest. \n",
    ">\n",
    ">**Random Forest Prototype:**  \n",
    "![random_forest_prototype.png](assets/random_forest_prototype.png)\n",
    ">\n",
    ">**Random Forest Mature:**  \n",
    "![random_forest_mature.png](assets/random_forest_mature.png)\n",
    ">\n",
    ">**Random Forest Regression:**  \n",
    "![random_forest_regression.png](assets/random_forest_regression.png)\n",
    ">\n",
    ">**Pro:**\n",
    ">- Both training and prediction are very fast, because of the simplicity of the underlying decision trees. In addition, both tasks can be straightforwardly parallelized, because the individual trees are entirely independent entities.\n",
    ">- The multiple trees allow for a probabilistic classification: a majority vote among estimators gives an estimate of the probability (accessed in Scikit-Learn with the predict_proba() method).\n",
    ">- The nonparametric model is extremely flexible, and can thus perform well on tasks that are under-fit by other estimators.\n",
    ">\n",
    ">**Con:**\n",
    ">- A primary disadvantage of random forests is that the results are not easily interpretable: that is, if you would like to draw conclusions about the meaning of the classification model, random forests may not be the best choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
